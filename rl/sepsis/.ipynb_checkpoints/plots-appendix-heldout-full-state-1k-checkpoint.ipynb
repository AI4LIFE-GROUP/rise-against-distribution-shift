{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Repeated evaluation w/full state observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.067095Z",
     "start_time": "2019-04-19T16:43:04.049687Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.753547Z",
     "start_time": "2019-04-19T16:43:04.068986Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sepsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4e37a4d7153d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounterfactual\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/crcs/rmdp/rl-mismatch/sepsis/cf/counterfactual.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msepsis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdptoolboxSrc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmdp\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmdptools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msepsis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgumbelTools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sepsis'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cf.counterfactual as cf\n",
    "import cf.utils as utils\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools as it\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "# Sepsis Simulator code\n",
    "from sepsisSimDiabetes.State import State\n",
    "from sepsisSimDiabetes.Action import Action\n",
    "from sepsisSimDiabetes.DataGenerator import DataGenerator\n",
    "import sepsisSimDiabetes.MDP as simulator \n",
    "\n",
    "import mdptoolboxSrc.mdp as mdptools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Avoid Type 3 fonts\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "figpath = \"./figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.753547Z",
     "start_time": "2019-04-19T16:43:04.068986Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_prefix = \"appendix-multiple-heldout-full-state-1k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.769134Z",
     "start_time": "2019-04-19T16:43:04.755230Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1  # Note this is not the only random seed, see the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.787255Z",
     "start_time": "2019-04-19T16:43:04.770642Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "NSIMSAMPS = 1000  # Samples to draw from the simulator\n",
    "NSTEPS = 20  # Max length of each trajectory\n",
    "NCFSAMPS = 1  # Counterfactual Samples per observed sample\n",
    "DISCOUNT_Pol = 0.99 # Used for computing optimal policies\n",
    "DISCOUNT = 1 # Used for computing actual reward\n",
    "PHYS_EPSILON = 0.05 # Used for sampling using physician pol as eps greedy\n",
    "\n",
    "PROB_DIAB = 0.2\n",
    "\n",
    "# Number of iterations to get error bars\n",
    "N_REPEAT_SAMPLING = 100\n",
    "NHELDOUT = 1000 # Heldout samples for WIS\n",
    "\n",
    "# These are properties of the simulator, do not change\n",
    "n_actions = Action.NUM_ACTIONS_TOTAL\n",
    "n_components = 2\n",
    "n_glucose_levels = 5\n",
    "\n",
    "# These are added as absorbing states\n",
    "# NOTE: Change to full states\n",
    "n_states_abs = State.NUM_FULL_STATES + 2\n",
    "discStateIdx = n_states_abs - 1\n",
    "deadStateIdx = n_states_abs - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.944514Z",
     "start_time": "2019-04-19T16:43:04.789044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the transition and reward matrix from file\n",
    "with open(\"./data/diab_txr_mats-replication.pkl\", \"rb\") as f:\n",
    "    mdict = pickle.load(f)\n",
    "\n",
    "tx_mat = mdict[\"tx_mat\"]\n",
    "r_mat = mdict[\"r_mat\"]\n",
    "p_mixture = np.array([1 - PROB_DIAB, PROB_DIAB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:05.064330Z",
     "start_time": "2019-04-19T16:43:04.946096Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag\n",
    "\n",
    "tx_mat_full = np.zeros((n_actions, State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
    "r_mat_full = np.zeros((n_actions, State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
    "\n",
    "for a in range(n_actions):\n",
    "    tx_mat_full[a, ...] = block_diag(tx_mat[0, a, ...], tx_mat[1, a,...])\n",
    "    r_mat_full[a, ...] = block_diag(r_mat[0, a, ...], r_mat[1, a, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:14.717995Z",
     "start_time": "2019-04-19T16:43:05.066210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.74 s, sys: 491 ms, total: 10.2 s\n",
      "Wall time: 6.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fullMDP = cf.MatrixMDP(tx_mat_full, r_mat_full)\n",
    "fullPol = fullMDP.policyIteration(discount=DISCOUNT_Pol, eval_type=1)\n",
    "\n",
    "physPolSoft = np.copy(fullPol)\n",
    "physPolSoft[physPolSoft == 1] = 1 - PHYS_EPSILON\n",
    "physPolSoft[physPolSoft == 0] = PHYS_EPSILON / (n_actions - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/physician_policy_st.pkl', 'wb') as fw:\n",
    "    pickle.dump(physPolSoft, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:14.761240Z",
     "start_time": "2019-04-19T16:43:14.740751Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_reward = []\n",
    "offpol_opt_reward_WIS_hard_train = []                         \n",
    "offpol_opt_reward_WIS_hard_ho = []                         \n",
    "offpol_opt_reward_mb = []\n",
    "true_rl_reward = []\n",
    "\n",
    "# We will save the detailed samples from the first run\n",
    "saved_material = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rl_policy(rl_policy, obs_samps, proj_lookup):\n",
    "\n",
    "    passes = True\n",
    "    # Check the observed actions for each state\n",
    "    obs_pol = np.zeros_like(rl_policy)\n",
    "    for eps_idx in range(NSIMSAMPS):\n",
    "        for time_idx in range(NSTEPS):\n",
    "            this_obs_action = int(obs_samps[eps_idx, time_idx, 1])\n",
    "            # Need to get projected state\n",
    "            if this_obs_action == -1:\n",
    "                continue\n",
    "            this_obs_state = proj_lookup[int(obs_samps[eps_idx, time_idx, 2])]\n",
    "            obs_pol[this_obs_state, this_obs_action] += 1\n",
    "\n",
    "    # Check if each RL action conforms to an observed action\n",
    "    for eps_idx in range(NSIMSAMPS):\n",
    "        for time_idx in range(NSTEPS):\n",
    "            this_full_state_unobserved = int(obs_samps[eps_idx, time_idx, 1])\n",
    "            this_obs_state = proj_lookup[this_full_state_unobserved]\n",
    "            this_obs_action = int(obs_samps[eps_idx, time_idx, 1])\n",
    "\n",
    "            if this_obs_action == -1:\n",
    "                continue\n",
    "            # This is key: In some of these trajectories, you die or get discharge.  \n",
    "            # In this case, no action is taken because the sequence has terminated, so there's nothing to compare the RL action to\n",
    "            true_death_states = r_mat[0, 0, 0, :] == -1\n",
    "            true_disch_states = r_mat[0, 0, 0, :] == 1\n",
    "            if np.logical_or(true_death_states, true_disch_states)[this_full_state_unobserved]:\n",
    "                continue\n",
    "\n",
    "            this_rl_action = rl_policy[proj_lookup[this_obs_state]].argmax()\n",
    "            if obs_pol[this_obs_state, this_rl_action] == 0:\n",
    "                print(\"Eps: {} \\t RL Action {} in State {} never observed\".format(\n",
    "                    int(time_idx / NSTEPS), this_rl_action, this_obs_state))\n",
    "                passes = False\n",
    "    return passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the projection matrix for obs->proj states\n",
    "\n",
    "# In this case, this is an identity matrix\n",
    "n_proj_states = n_states_abs\n",
    "proj_matrix = np.eye(n_states_abs)\n",
    "\n",
    "proj_matrix = proj_matrix.astype(int)\n",
    "\n",
    "proj_lookup = proj_matrix.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgen = DataGenerator()\n",
    "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
    "   NSIMSAMPS, NSTEPS, policy=physPolSoft, policy_idx_type='full', \n",
    "   output_state_idx_type='full', \n",
    "   p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
    "\n",
    "obs_samps = utils.format_dgen_samps(\n",
    "   states, actions, rewards, diab, NSTEPS, NSIMSAMPS)\n",
    "\n",
    "emp_tx_mat_full = np.copy(emp_tx_totals)\n",
    "emp_r_mat_full = np.copy(emp_r_totals)\n",
    "\n",
    "# (2) Add new aborbing states, and a new est_tx_mat with Absorbing states\n",
    "death_states = (emp_r_mat_full.sum(axis=0).sum(axis=0) < 0)\n",
    "disch_states = (emp_r_mat_full.sum(axis=0).sum(axis=0) > 0)\n",
    "\n",
    "est_tx_cts_abs = np.zeros((n_actions, n_states_abs, n_states_abs))\n",
    "est_tx_cts_abs[:, :-2, :-2] = np.copy(emp_tx_mat_full)\n",
    "\n",
    "death_states = np.concatenate([death_states, np.array([True, False])])\n",
    "disch_states = np.concatenate([disch_states, np.array([False, True])])\n",
    "assert est_tx_cts_abs[:, death_states, :].sum() == 0\n",
    "assert est_tx_cts_abs[:, disch_states, :].sum() == 0\n",
    "\n",
    "est_tx_cts_abs[:, death_states, deadStateIdx] = 1\n",
    "est_tx_cts_abs[:, disch_states, discStateIdx] = 1\n",
    "\n",
    "# (3) Project the new est_tx_cts_abs to the reduced state space\n",
    "# PASS IN THIS CASE\n",
    "proj_tx_cts = np.copy(est_tx_cts_abs)\n",
    "proj_tx_mat = np.zeros_like(proj_tx_cts)\n",
    "\n",
    "# Normalize\n",
    "nonzero_idx = proj_tx_cts.sum(axis=-1) != 0\n",
    "proj_tx_mat[nonzero_idx] = proj_tx_cts[nonzero_idx]\n",
    "\n",
    "proj_tx_mat[nonzero_idx] /= proj_tx_mat[nonzero_idx].sum(axis=-1, keepdims=True)\n",
    "\n",
    "############ Construct the reward matrix, which is known ##################\n",
    "proj_r_mat = np.zeros((n_actions, n_proj_states, n_proj_states))\n",
    "proj_r_mat[..., -2] = -1\n",
    "proj_r_mat[..., -1] = 1\n",
    "\n",
    "proj_r_mat[..., -2, -2] = 0 # No reward once in aborbing state\n",
    "proj_r_mat[..., -1, -1] = 0\n",
    "\n",
    "############ Construct the empirical prior on the initial state ##################\n",
    "initial_state_arr = np.copy(states[:, 0, 0])\n",
    "initial_state_counts = np.zeros((n_states_abs,1))\n",
    "for i in range(initial_state_arr.shape[0]):\n",
    "    initial_state_counts[initial_state_arr[i]] += 1\n",
    "\n",
    "# Project initial state counts to new states\n",
    "proj_state_counts = proj_matrix.T.dot(initial_state_counts).T\n",
    "proj_p_initial_state = proj_state_counts / proj_state_counts.sum()\n",
    "\n",
    "# Check projection is still identity\n",
    "assert np.all(proj_state_counts.T == initial_state_counts)\n",
    "\n",
    "# Because some SA pairs are never observed, assume they cause instant death\n",
    "zero_sa_pairs = proj_tx_mat.sum(axis=-1) == 0\n",
    "proj_tx_mat[zero_sa_pairs, -2] = 1  # Always insta-death if you take a never-taken action\n",
    "\n",
    "# Construct an extra axis for the mixture component, of which there is only one\n",
    "projMDP = cf.MatrixMDP(proj_tx_mat, proj_r_mat, \n",
    "                       p_initial_state=proj_p_initial_state)\n",
    "try:\n",
    "    RlPol = projMDP.policyIteration(discount=DISCOUNT_Pol)\n",
    "except:\n",
    "    assert np.allclose(proj_tx_mat.sum(axis=-1), 1)\n",
    "    RlPol = projMDP.policyIteration(discount=DISCOUNT_Pol, skip_check=True)\n",
    "    \n",
    "# Get the true RL reward as a sanity check\n",
    "# Note that the RL policy includes actions for \"death\" and \"discharge\" absorbing states, which we ignore by taking [:-2, :]\n",
    "NSIMSAMPS_RL = NSIMSAMPS\n",
    "states_rl, actions_rl, lengths_rl, rewards_rl, diab_rl, _, _ = dgen.simulate(\n",
    "   NSIMSAMPS_RL, NSTEPS, policy=RlPol[:-2, :], \n",
    "   policy_idx_type='full', # Note the difference \n",
    "   p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='RL Policy Simulation')\n",
    "\n",
    "obs_samps_rlpol = utils.format_dgen_samps(\n",
    "   states_rl, actions_rl, rewards_rl, diab_rl, NSTEPS, NSIMSAMPS_RL)\n",
    "\n",
    "this_true_rl_reward = cf.eval_on_policy(\n",
    "    obs_samps_rlpol, discount=DISCOUNT, \n",
    "    bootstrap=False)  # Need a second axis to concat later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/rl_policy.pkl', 'wb') as fw:\n",
    "    pickle.dump(RlPol, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "NSIMSAMPS = 10000\n",
    "dgen = DataGenerator()\n",
    "states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
    "   NSIMSAMPS, NSTEPS, policy=RlPol[:-2, :], policy_idx_type='full', \n",
    "   output_state_idx_type='full', \n",
    "   p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
    "\n",
    "obs_samps = utils.format_dgen_samps(\n",
    "   states, actions, rewards, diab, NSTEPS, NSIMSAMPS)\n",
    "    \n",
    "emp_tx_mat_full = np.copy(emp_tx_totals)\n",
    "emp_r_mat_full = np.copy(emp_r_totals)\n",
    "\n",
    "# (2) Add new aborbing states, and a new est_tx_mat with Absorbing states\n",
    "death_states = (emp_r_mat_full.sum(axis=0).sum(axis=0) < 0)\n",
    "disch_states = (emp_r_mat_full.sum(axis=0).sum(axis=0) > 0)\n",
    "\n",
    "# est_tx_cts_abs = np.zeros((n_actions, n_states_abs, n_states_abs))\n",
    "# est_tx_cts_abs[:, :-2, :-2] = np.copy(emp_tx_mat_full)\n",
    "\n",
    "# death_states = np.concatenate([death_states, np.array([True, False])])\n",
    "# disch_states = np.concatenate([disch_states, np.array([False, True])])\n",
    "# assert est_tx_cts_abs[:, death_states, :].sum() == 0\n",
    "# assert est_tx_cts_abs[:, disch_states, :].sum() == 0\n",
    "\n",
    "# est_tx_cts_abs[:, death_states, deadStateIdx] = 1\n",
    "# est_tx_cts_abs[:, disch_states, discStateIdx] = 1\n",
    "\n",
    "# # (3) Project the new est_tx_cts_abs to the reduced state space\n",
    "# # PASS IN THIS CASE\n",
    "# proj_tx_cts = np.copy(est_tx_cts_abs)\n",
    "# proj_tx_mat = np.zeros_like(proj_tx_cts)\n",
    "\n",
    "# # Normalize\n",
    "# nonzero_idx = proj_tx_cts.sum(axis=-1) != 0\n",
    "# proj_tx_mat[nonzero_idx] = proj_tx_cts[nonzero_idx]\n",
    "\n",
    "# proj_tx_mat[nonzero_idx] /= proj_tx_mat[nonzero_idx].sum(axis=-1, keepdims=True)\n",
    "\n",
    "# ############ Construct the reward matrix, which is known ##################\n",
    "# proj_r_mat = np.zeros((n_actions, n_proj_states, n_proj_states))\n",
    "# proj_r_mat[..., -2] = -1\n",
    "# proj_r_mat[..., -1] = 1\n",
    "\n",
    "# proj_r_mat[..., -2, -2] = 0 # No reward once in aborbing state\n",
    "# proj_r_mat[..., -1, -1] = 0\n",
    "\n",
    "# ############ Construct the empirical prior on the initial state ##################\n",
    "# initial_state_arr = np.copy(states[:, 0, 0])\n",
    "# initial_state_counts = np.zeros((n_states_abs,1))\n",
    "# for i in range(initial_state_arr.shape[0]):\n",
    "#     initial_state_counts[initial_state_arr[i]] += 1\n",
    "\n",
    "# # Project initial state counts to new states\n",
    "# proj_state_counts = proj_matrix.T.dot(initial_state_counts).T\n",
    "# proj_p_initial_state = proj_state_counts / proj_state_counts.sum()\n",
    "\n",
    "# # Check projection is still identity\n",
    "# assert np.all(proj_state_counts.T == initial_state_counts)\n",
    "\n",
    "# # Because some SA pairs are never observed, assume they cause instant death\n",
    "# zero_sa_pairs = proj_tx_mat.sum(axis=-1) == 0\n",
    "# proj_tx_mat[zero_sa_pairs, -2] = 1  # Always insta-death if you take a never-taken action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_tx_cts = np.copy(emp_tx_mat_full)\n",
    "proj_tx_mat = np.zeros_like(proj_tx_cts)\n",
    "\n",
    "# Normalize\n",
    "nonzero_idx = proj_tx_cts.sum(axis=-1) != 0\n",
    "proj_tx_mat[nonzero_idx] = proj_tx_cts[nonzero_idx]\n",
    "\n",
    "proj_tx_mat[nonzero_idx] /= proj_tx_mat[nonzero_idx].sum(axis=-1, keepdims=True)\n",
    "\n",
    "proj_r_cts = np.copy(emp_r_mat_full)\n",
    "proj_r_mat = np.zeros_like(proj_r_cts)\n",
    "\n",
    "proj_r_mat[:,:,death_states] = -1\n",
    "proj_r_mat[:,:,disch_states] = 1\n",
    "\n",
    "# # Normalize\n",
    "# nonzero_idx = proj_r_cts.sum(axis=-1) != 0\n",
    "# proj_r_mat[nonzero_idx] = proj_r_cts[nonzero_idx]\n",
    "\n",
    "# proj_r_mat[nonzero_idx] /= proj_tx_cts[nonzero_idx].sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tx_tr_obs_rlpol.pkl', 'wb') as fw:\n",
    "    pickle.dump((proj_tx_mat, proj_r_mat, obs_samps), fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of cf.counterfactual failed: Traceback (most recent call last):\n",
      "  File \"/Users/harvineetsingh/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/harvineetsingh/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/harvineetsingh/opt/anaconda3/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/Users/harvineetsingh/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/harvineetsingh/Downloads/crcs/rmdp/rl-mismatch/sepsis/cf/counterfactual.py\", line 5, in <module>\n",
      "    from ..mdptoolboxSrc import mdp as mdptools\n",
      "ValueError: attempted relative import beyond top-level package\n",
      "]\n",
      "[autoreload of cf.utils failed: Traceback (most recent call last):\n",
      "  File \"/Users/harvineetsingh/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/harvineetsingh/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/harvineetsingh/opt/anaconda3/lib/python3.8/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/Users/harvineetsingh/opt/anaconda3/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/Users/harvineetsingh/Downloads/crcs/rmdp/rl-mismatch/sepsis/cf/utils.py\", line 3, in <module>\n",
      "    from ..sepsisSimDiabetes.State import State\n",
      "ValueError: attempted relative import beyond top-level package\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "with open('data/tx_tr_obs_rlpol_seed{}.pkl'.format(SEED), 'rb') as fr:\n",
    "    proj_tx_mat, proj_r_mat, obs_samps = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0119\n"
     ]
    }
   ],
   "source": [
    "# Note that the RL policy includes actions for \"death\" and \"discharge\" absorbing states, which we ignore by taking [:-2, :]\n",
    "NSIMSAMPS_RL = NSIMSAMPS\n",
    "states_rl, actions_rl, lengths_rl, rewards_rl, diab_rl, _, _ = dgen.simulate(\n",
    "   NSIMSAMPS_RL, NSTEPS, policy=RlPol[:-2, :], \n",
    "   policy_idx_type='full', # Note the difference \n",
    "   p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='RL Policy Simulation')\n",
    "\n",
    "obs_samps_rlpol = utils.format_dgen_samps(\n",
    "   states_rl, actions_rl, rewards_rl, diab_rl, NSTEPS, NSIMSAMPS_RL)\n",
    "\n",
    "this_true_rl_reward = cf.eval_on_policy(\n",
    "        obs_samps_rlpol, discount=DISCOUNT, \n",
    "        bootstrap=False)  # Need a second axis to concat later\n",
    "\n",
    "print(this_true_rl_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes, n_maxsteps, _ = obs_samps.shape\n",
    "P_sa = np.zeros((Action.NUM_ACTIONS_TOTAL,\n",
    "                State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
    "P_glucose_sa = np.zeros((Action.NUM_ACTIONS_TOTAL,\n",
    "                State.NUM_FULL_STATES, n_glucose_levels))\n",
    "P_rest_glucose_sa = np.zeros((n_glucose_levels, Action.NUM_ACTIONS_TOTAL,\n",
    "                State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
    "\n",
    "for i_episode in range(n_episodes):\n",
    "    for i_step in range(n_maxsteps):\n",
    "        curr_a = int(obs_samps[i_episode, i_step, 1])\n",
    "        curr_s = int(obs_samps[i_episode, i_step, 2])\n",
    "        next_s = int(obs_samps[i_episode, i_step, 3])\n",
    "        if curr_a==-1: # terminal state\n",
    "            break\n",
    "        this_curr_state = State(state_idx = curr_s, idx_type='full', \n",
    "                       diabetic_idx = 1) # diabetic_idx does not matter, req argument\n",
    "        this_next_state = State(state_idx = next_s, idx_type='full', \n",
    "                       diabetic_idx = 1) # diabetic_idx does not matter, req argument\n",
    "        \n",
    "        P_sa[curr_a,curr_s,next_s]+=1\n",
    "        P_glucose_sa[curr_a,curr_s,this_next_state.glucose_state]+=1\n",
    "        P_rest_glucose_sa[this_next_state.glucose_state,curr_a,curr_s,next_s]+=1\n",
    "\n",
    "P_sa_mat = np.zeros_like(P_sa)\n",
    "P_glucose_sa_mat = np.zeros_like(P_glucose_sa)\n",
    "P_rest_glucose_sa_mat = np.zeros_like(P_rest_glucose_sa)\n",
    "\n",
    "# Normalize\n",
    "nonzero_idx = P_sa.sum(axis=-1) != 0\n",
    "P_sa_mat[nonzero_idx] = P_sa[nonzero_idx]\n",
    "P_sa_mat[nonzero_idx] /= P_sa_mat[nonzero_idx].sum(axis=-1, keepdims=True)\n",
    "\n",
    "nonzero_idx = P_glucose_sa.sum(axis=-1) != 0\n",
    "P_glucose_sa_mat[nonzero_idx] = P_glucose_sa[nonzero_idx]\n",
    "P_glucose_sa_mat[nonzero_idx] /= P_glucose_sa_mat[nonzero_idx].sum(axis=-1, keepdims=True)\n",
    "\n",
    "nonzero_idx = P_rest_glucose_sa.sum(axis=-1) != 0\n",
    "P_rest_glucose_sa_mat[nonzero_idx] = P_rest_glucose_sa[nonzero_idx]\n",
    "P_rest_glucose_sa_mat[nonzero_idx] /= P_rest_glucose_sa_mat[nonzero_idx].sum(axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 1440, 1440), (8, 1442, 1442))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_sa_mat.shape, proj_tx_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(P_sa_mat,proj_tx_mat[:,:-2,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 1440, 1440), (8, 1440, 5), (5, 8, 1440, 1440))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_sa_mat.shape, P_glucose_sa_mat.shape, P_rest_glucose_sa_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5,), (1440,))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_glucose_sa_mat[0,97,:].shape,np.dot(P_glucose_sa_mat[0,97,:],P_rest_glucose_sa_mat[:,0,97,:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_sa_mat_comp = np.zeros_like(P_sa_mat)\n",
    "\n",
    "for i_g in range(P_rest_glucose_sa_mat.shape[0]):\n",
    "    for i_a in range(P_rest_glucose_sa_mat.shape[1]):\n",
    "        for i_cs in range(P_rest_glucose_sa_mat.shape[2]):\n",
    "            P_sa_mat_comp[i_a,i_cs,:] = P_glucose_sa_mat[i_a,i_cs,:].dot(P_rest_glucose_sa_mat[:,i_a,i_cs,:])\n",
    "            \n",
    "np.allclose(P_sa_mat_comp, P_sa_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.28571429, 0.71428571, 0.        , 0.        ],\n",
       "       [0.92079208, 0.07920792, 0.        , 0.        , 0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_glucose_sa_mat[P_glucose_sa_mat.sum(axis=-1) != 0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_rest_glucose_sa_mat[P_rest_glucose_sa_mat.sum(axis=-1) != 0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de78849bdff24b528eb3da1b20db90f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0 -0.03979999999999999 0.020836506425022407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46e1fc3f1d048d9a7a38c38b403767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.1 -0.059899999999999995 0.03142435361308169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf168d3a67c4c8f8f4150e3cbe08c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.2 -0.0783 0.008246817568007674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b95509985d464594d5a8a9e9b9b2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.30000000000000004 -0.10010000000000001 0.025967094562156928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f99263d7c0b43f4ac9bc3d5bb155d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.4 -0.1006 0.02605839595984373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13685cd3d0194e7bb1833a7e91a4c6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5 -0.10640000000000001 0.02190981515211847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f9c8008a124775bfcaa2e03a583a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.6000000000000001 -0.10840000000000001 0.023618636709175234\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec54b841afb456da3623cbdc6bc9913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7000000000000001 -0.1111 0.024549745416195253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db9ff7cee6c4a01b414fab6c58677e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.8 -0.1363 0.031087135603011098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b3b5c156744a13b3a54fdd5b8a9270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.9 -0.149 0.026072974513852456\n"
     ]
    }
   ],
   "source": [
    "N_REPEAT_SAMPLING=10\n",
    "for p_diab in np.arange(0,1,0.1):\n",
    "    PROB_DIAB = p_diab\n",
    "    true_rl_reward = np.zeros(N_REPEAT_SAMPLING)\n",
    "    for it in tqdm(range(N_REPEAT_SAMPLING), desc=\"Outer Loop\"):\n",
    "        np.random.seed(it)\n",
    "        NSIMSAMPS_RL = NSIMSAMPS\n",
    "        states_rl, actions_rl, lengths_rl, rewards_rl, diab_rl, _, _ = dgen.simulate(\n",
    "           NSIMSAMPS_RL, NSTEPS, policy=RlPol[:-2, :], \n",
    "           policy_idx_type='full', # Note the difference \n",
    "           p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='RL Policy Simulation')\n",
    "\n",
    "        obs_samps_rlpol = utils.format_dgen_samps(\n",
    "           states_rl, actions_rl, rewards_rl, diab_rl, NSTEPS, NSIMSAMPS_RL)\n",
    "\n",
    "        this_true_rl_reward = cf.eval_on_policy(\n",
    "            obs_samps_rlpol, discount=DISCOUNT, \n",
    "            bootstrap=False)  # Need a second axis to concat later\n",
    "        true_rl_reward[it] = this_true_rl_reward\n",
    "    print(p_diab, true_rl_reward.mean(), true_rl_reward.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbede46c70e6476fba45c275ad7f7242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0 0.30179999999999996 0.014489996549343957\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbbde3a9830433db6ce09da43261278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.1 0.30419999999999997 0.012464349160706318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0d2bd14cbd4bf6bc7a2d3d5d1f660e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.2 0.3083 0.018926436537288264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d6f4b8e1954649a192782f79b8568c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.30000000000000004 0.32049999999999995 0.022348378017207418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9f6edc46b94a34b949babf21da895c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.4 0.3262999999999999 0.02463757293241362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdcc0f28f7141e888538d3c1aa65891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5 0.32439999999999997 0.01900631474010678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966e2df803884e418042d28ef40ac693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.6000000000000001 0.3409 0.026334198298030645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196b5e4b6e13435aae58c84726f90932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7000000000000001 0.3315 0.022213734490175215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fa6bfaf5d94dafb11354059085649e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.8 0.3431 0.023045389994530353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc315fd0ed6a41728ddb372f5c1c334c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Outer Loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.9 0.33859999999999996 0.02478386571945547\n"
     ]
    }
   ],
   "source": [
    "N_REPEAT_SAMPLING=10\n",
    "for p_diab in np.arange(0,1,0.1):\n",
    "    PROB_DIAB = p_diab\n",
    "    obs_reward = np.zeros(N_REPEAT_SAMPLING)\n",
    "    for it in tqdm(range(N_REPEAT_SAMPLING), desc=\"Outer Loop\"):\n",
    "        np.random.seed(it)\n",
    "        dgen = DataGenerator()\n",
    "        states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
    "           NSIMSAMPS, NSTEPS, policy=physPolSoft, policy_idx_type='full', \n",
    "           output_state_idx_type='full', \n",
    "           p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
    "\n",
    "        obs_samps = utils.format_dgen_samps(\n",
    "           states, actions, rewards, diab, NSTEPS, NSIMSAMPS)\n",
    "\n",
    "        this_obs_reward = cf.eval_on_policy(\n",
    "                obs_samps, discount=DISCOUNT, \n",
    "                bootstrap=False)\n",
    "        obs_reward[it] = this_obs_reward\n",
    "    print(p_diab, obs_reward.mean(), obs_reward.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_state = State(state_idx = obs_samps[0,0,2], idx_type='full', \n",
    "                       diabetic_idx = 1)\n",
    "this_state.get_state_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_state.glucose_state=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 4, 0, 0, 0]), 392)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_state.get_state_vector(),this_state.get_state_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "State(state_idx = this_state.get_state_idx(), idx_type='full', \n",
    "                       diabetic_idx = 1).glucose_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "State(state_idx = 721, idx_type='full', \n",
    "                       diabetic_idx = 1).diabetic_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.eval_on_policy(\n",
    "    obs_samps, discount=DISCOUNT, \n",
    "    bootstrap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -1,   16,   17,   40,   48,   49,   51,   56,   57,   59,   80,\n",
       "         84,   88,   89,   96,   98,   99,  120,  121,  122,  124,  125,\n",
       "        128,  129,  131,  132,  133,  136,  137,  138,  139,  141,  144,\n",
       "        147,  149,  152,  157,  160,  162,  177,  178,  179,  200,  201,\n",
       "        202,  208,  209,  211,  216,  217,  218,  219,  227,  240,  248,\n",
       "        252,  256,  263,  280,  281,  284,  285,  288,  289,  291,  292,\n",
       "        293,  296,  297,  301,  303,  304,  305,  309,  311,  312,  319,\n",
       "        320,  321,  322,  324,  328,  329,  332,  333,  336,  337,  339,\n",
       "        340,  341,  343,  344,  348,  349,  352,  356,  357,  360,  361,\n",
       "        362,  364,  365,  368,  369,  371,  372,  373,  374,  376,  377,\n",
       "        378,  379,  380,  381,  383,  384,  385,  386,  387,  388,  389,\n",
       "        391,  392,  393,  396,  397,  399,  400,  401,  402,  404,  408,\n",
       "        409,  412,  414,  416,  418,  419,  423,  426,  440,  441,  442,\n",
       "        444,  445,  448,  449,  451,  452,  453,  454,  456,  457,  458,\n",
       "        459,  461,  463,  464,  465,  466,  467,  469,  471,  472,  473,\n",
       "        477,  488,  496,  497,  520,  521,  528,  529,  531,  533,  536,\n",
       "        541,  543,  560,  561,  562,  568,  576,  577,  584,  600,  601,\n",
       "        602,  605,  608,  609,  611,  613,  616,  617,  621,  623,  624,\n",
       "        625,  626,  627,  629,  632,  637,  640,  642,  648,  663,  680,\n",
       "        681,  682,  688,  689,  691,  693,  696,  697,  699,  701,  703,\n",
       "        704,  705,  706,  707,  713,  752,  760,  776,  777,  784,  785,\n",
       "        793,  816,  824,  832,  840,  841,  845,  848,  849,  856,  857,\n",
       "        859,  861,  864,  865,  866,  867,  872,  873,  874,  898,  904,\n",
       "        906,  914,  933,  936,  938,  944,  945,  947,  952,  954,  992,\n",
       "       1016, 1023, 1024, 1029, 1032, 1039, 1040, 1048, 1052, 1053, 1056,\n",
       "       1060, 1061, 1064, 1068, 1069, 1072, 1076, 1077, 1080, 1083, 1085,\n",
       "       1087, 1088, 1089, 1092, 1093, 1096, 1097, 1100, 1101, 1103, 1104,\n",
       "       1105, 1108, 1109, 1111, 1112, 1113, 1116, 1117, 1119, 1128, 1136,\n",
       "       1144, 1146, 1152, 1154, 1160, 1163, 1165, 1167, 1168, 1170, 1171,\n",
       "       1173, 1175, 1176, 1177, 1178, 1179, 1181, 1183, 1184, 1185, 1186,\n",
       "       1187, 1189, 1190, 1191, 1192, 1193, 1197, 1199, 1232, 1256, 1264,\n",
       "       1271, 1272, 1288, 1296, 1304, 1312, 1325, 1328, 1333, 1336, 1341,\n",
       "       1344, 1349, 1351, 1352, 1357, 1360, 1378, 1384, 1392, 1407, 1415,\n",
       "       1416, 1418, 1419, 1424, 1425, 1426, 1427, 1430, 1431, 1432, 1433,\n",
       "       1439])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 1442, 1442), (8, 1442, 1442))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj_tx_mat.shape, proj_r_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.179"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab[:,0,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T17:12:19.338614Z",
     "start_time": "2019-04-19T16:43:14.787770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22e4585b2ea47c6b29f9b1a67e59545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Outer Loop', style=ProgressStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for it in tqdm(range(N_REPEAT_SAMPLING), desc=\"Outer Loop\"):\n",
    "    np.random.seed(it)\n",
    "    dgen = DataGenerator()\n",
    "    states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
    "       NSIMSAMPS, NSTEPS, policy=physPolSoft, policy_idx_type='full', \n",
    "       output_state_idx_type='full', \n",
    "       p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
    "\n",
    "    obs_samps = utils.format_dgen_samps(\n",
    "       states, actions, rewards, diab, NSTEPS, NSIMSAMPS)\n",
    "\n",
    "    emp_tx_mat_full = np.copy(emp_tx_totals)\n",
    "    emp_r_mat_full = np.copy(emp_r_totals)\n",
    "\n",
    "    # (2) Add new aborbing states, and a new est_tx_mat with Absorbing states\n",
    "    death_states = (emp_r_mat_full.sum(axis=0).sum(axis=0) < 0)\n",
    "    disch_states = (emp_r_mat_full.sum(axis=0).sum(axis=0) > 0)\n",
    "\n",
    "    est_tx_cts_abs = np.zeros((n_actions, n_states_abs, n_states_abs))\n",
    "    est_tx_cts_abs[:, :-2, :-2] = np.copy(emp_tx_mat_full)\n",
    "\n",
    "    death_states = np.concatenate([death_states, np.array([True, False])])\n",
    "    disch_states = np.concatenate([disch_states, np.array([False, True])])\n",
    "    assert est_tx_cts_abs[:, death_states, :].sum() == 0\n",
    "    assert est_tx_cts_abs[:, disch_states, :].sum() == 0\n",
    "\n",
    "    est_tx_cts_abs[:, death_states, deadStateIdx] = 1\n",
    "    est_tx_cts_abs[:, disch_states, discStateIdx] = 1\n",
    "\n",
    "    # (3) Project the new est_tx_cts_abs to the reduced state space\n",
    "    # PASS IN THIS CASE\n",
    "    proj_tx_cts = np.copy(est_tx_cts_abs)\n",
    "    proj_tx_mat = np.zeros_like(proj_tx_cts)\n",
    "\n",
    "    # Normalize\n",
    "    nonzero_idx = proj_tx_cts.sum(axis=-1) != 0\n",
    "    proj_tx_mat[nonzero_idx] = proj_tx_cts[nonzero_idx]\n",
    "\n",
    "    proj_tx_mat[nonzero_idx] /= proj_tx_mat[nonzero_idx].sum(axis=-1, keepdims=True)\n",
    "\n",
    "    ############ Construct the reward matrix, which is known ##################\n",
    "    proj_r_mat = np.zeros((n_actions, n_proj_states, n_proj_states))\n",
    "    proj_r_mat[..., -2] = -1\n",
    "    proj_r_mat[..., -1] = 1\n",
    "\n",
    "    proj_r_mat[..., -2, -2] = 0 # No reward once in aborbing state\n",
    "    proj_r_mat[..., -1, -1] = 0\n",
    "\n",
    "    ############ Construct the empirical prior on the initial state ##################\n",
    "    initial_state_arr = np.copy(states[:, 0, 0])\n",
    "    initial_state_counts = np.zeros((n_states_abs,1))\n",
    "    for i in range(initial_state_arr.shape[0]):\n",
    "        initial_state_counts[initial_state_arr[i]] += 1\n",
    "\n",
    "    # Project initial state counts to new states\n",
    "    proj_state_counts = proj_matrix.T.dot(initial_state_counts).T\n",
    "    proj_p_initial_state = proj_state_counts / proj_state_counts.sum()\n",
    "\n",
    "    # Check projection is still identity\n",
    "    assert np.all(proj_state_counts.T == initial_state_counts)\n",
    "\n",
    "    # Because some SA pairs are never observed, assume they cause instant death\n",
    "    zero_sa_pairs = proj_tx_mat.sum(axis=-1) == 0\n",
    "    proj_tx_mat[zero_sa_pairs, -2] = 1  # Always insta-death if you take a never-taken action\n",
    "\n",
    "    # Construct an extra axis for the mixture component, of which there is only one\n",
    "    projMDP = cf.MatrixMDP(proj_tx_mat, proj_r_mat, \n",
    "                           p_initial_state=proj_p_initial_state)\n",
    "    try:\n",
    "        RlPol = projMDP.policyIteration(discount=DISCOUNT_Pol)\n",
    "    except:\n",
    "        assert np.allclose(proj_tx_mat.sum(axis=-1), 1)\n",
    "        RlPol = projMDP.policyIteration(discount=DISCOUNT_Pol, skip_check=True)\n",
    "\n",
    "    # Estimate the observed policy\n",
    "    obs_pol_proj = proj_tx_cts.sum(axis=-1)  # Sum over the \"to\" state\n",
    "    obs_pol_proj = obs_pol_proj.T # Switch from (a, s) to (s, a)\n",
    "    obs_states = obs_pol_proj.sum(axis=-1) > 0 # Observed \"from\" states\n",
    "\n",
    "    obs_pol_proj[obs_states] /= obs_pol_proj[obs_states].sum(axis=-1, keepdims=True)\n",
    "\n",
    "    # Check if we always observe the RL policy in the non-absorbing states\n",
    "    prop_rl_obs = (obs_pol_proj[:-2, :][RlPol[:-2, :]==1] > 0).mean()\n",
    "    if prop_rl_obs < 1:\n",
    "        assert check_rl_policy(RlPol, obs_samps, proj_lookup), 'RL policy validation failed'\n",
    "\n",
    "    def projection_func(obs_state_idx):\n",
    "        if obs_state_idx == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return proj_lookup[obs_state_idx]\n",
    "\n",
    "    proj_f = np.vectorize(projection_func)\n",
    "    states_proj = proj_f(states)\n",
    "    assert states_proj.shape == states.shape\n",
    "\n",
    "    obs_samps_proj = utils.format_dgen_samps(\n",
    "       states_proj, actions, rewards, diab, NSTEPS, NSIMSAMPS)\n",
    "\n",
    "    # Again, projection is identity function\n",
    "    assert np.all(obs_samps_proj == obs_samps)\n",
    "\n",
    "    # Get the true RL reward as a sanity check\n",
    "    # Note that the RL policy includes actions for \"death\" and \"discharge\" absorbing states, which we ignore by taking [:-2, :]\n",
    "    NSIMSAMPS_RL = NSIMSAMPS\n",
    "    states_rl, actions_rl, lengths_rl, rewards_rl, diab_rl, _, _ = dgen.simulate(\n",
    "       NSIMSAMPS_RL, NSTEPS, policy=RlPol[:-2, :], \n",
    "       policy_idx_type='full', # Note the difference \n",
    "       p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='RL Policy Simulation')\n",
    "\n",
    "    obs_samps_rlpol = utils.format_dgen_samps(\n",
    "       states_rl, actions_rl, rewards_rl, diab_rl, NSTEPS, NSIMSAMPS_RL)\n",
    "\n",
    "    this_true_rl_reward = cf.eval_on_policy(\n",
    "        obs_samps_rlpol, discount=DISCOUNT, \n",
    "        bootstrap=False)  # Need a second axis to concat later\n",
    "\n",
    "    # This is the observed reward from the samples given\n",
    "    this_obs_reward = cf.eval_on_policy(\n",
    "        obs_samps_proj, discount=DISCOUNT, \n",
    "        bootstrap=False)\n",
    "\n",
    "   # This is the off-policy reward using WIS\n",
    "    this_offpol_opt_reward_WIS_hard_train, this_wis_samps, this_wis_ct = cf.eval_wis(\n",
    "        obs_samps_proj, discount=DISCOUNT,\n",
    "        bootstrap=False,\n",
    "        obs_policy=obs_pol_proj, new_policy=RlPol)\n",
    "\n",
    "    # Draw samples from the MDP under the new policy to get a model-based estimate of reward\n",
    "    BSampler = cf.BatchSampler(mdp=projMDP)\n",
    "    this_mb_samples_opt = BSampler.on_policy_sample(\n",
    "        policy=RlPol, n_steps=NSTEPS, n_samps=NSIMSAMPS_RL, \n",
    "        use_tqdm=False) #, tqdm_desc='Model-Based OPE') \n",
    "\n",
    "    this_offpol_opt_reward_mb = cf.eval_on_policy(\n",
    "        this_mb_samples_opt, discount=DISCOUNT,\n",
    "        bootstrap=False)\n",
    "    \n",
    "    ###################################################\n",
    "    # Construct the held-out samples, freshly each time\n",
    "    ###################################################\n",
    "    ho_dgen = DataGenerator()\n",
    "    ho_states, ho_actions, ho_lengths, ho_rewards, ho_diab, ho_emp_tx_totals, ho_emp_r_totals = ho_dgen.simulate(\n",
    "       NHELDOUT, NSTEPS, policy=physPolSoft, policy_idx_type='full', \n",
    "       output_state_idx_type='full', \n",
    "       p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
    "\n",
    "    ho_obs_samps = utils.format_dgen_samps(\n",
    "       ho_states, ho_actions, ho_rewards, ho_diab, NSTEPS, NHELDOUT)\n",
    "\n",
    "    ho_emp_tx_mat = np.copy(ho_emp_tx_totals)\n",
    "    ho_emp_r_mat = np.copy(ho_emp_r_totals)\n",
    "\n",
    "    ############## Construct the Transition Matrix w/proj states ##############\n",
    "    ho_proj_tx_cts = np.zeros((n_actions, n_proj_states, n_proj_states))\n",
    "    ho_proj_tx_mat = np.zeros_like(ho_proj_tx_cts)\n",
    "\n",
    "    ho_est_tx_cts = np.copy(ho_emp_tx_mat)\n",
    "    assert ho_est_tx_cts.ndim == 3\n",
    "\n",
    "    # (2) Add new aborbing states, and a new est_tx_mat with Absorbing states\n",
    "    ho_death_states = (ho_emp_r_mat.sum(axis=0).sum(axis=0) < 0)\n",
    "    ho_disch_states = (ho_emp_r_mat.sum(axis=0).sum(axis=0) > 0)\n",
    "\n",
    "    ho_est_tx_cts_abs = np.zeros((n_actions, n_states_abs, n_states_abs))\n",
    "    ho_est_tx_cts_abs[:, :-2, :-2] = np.copy(ho_est_tx_cts)\n",
    "\n",
    "    ho_death_states = np.concatenate([ho_death_states, np.array([True, False])])\n",
    "    ho_disch_states = np.concatenate([ho_disch_states, np.array([False, True])])\n",
    "    assert ho_est_tx_cts_abs[:, ho_death_states, :].sum() == 0\n",
    "    assert ho_est_tx_cts_abs[:, ho_disch_states, :].sum() == 0\n",
    "\n",
    "    ho_est_tx_cts_abs[:, ho_death_states, deadStateIdx] = 1\n",
    "    ho_est_tx_cts_abs[:, ho_disch_states, discStateIdx] = 1\n",
    "\n",
    "    # (3) Project the new est_tx_cts_abs to the reduced state space\n",
    "    for a in range(n_actions):\n",
    "        ho_proj_tx_cts[a] = proj_matrix.T.dot(ho_est_tx_cts_abs[a]).dot(proj_matrix)\n",
    "\n",
    "    # Estimate the observed policy\n",
    "    ho_obs_pol_proj = ho_proj_tx_cts.sum(axis=-1)  # Sum over the \"to\" state\n",
    "    ho_obs_pol_proj = ho_obs_pol_proj.T # Switch from (a, s) to (s, a)\n",
    "    ho_obs_states = ho_obs_pol_proj.sum(axis=-1) > 0 # Observed \"from\" states\n",
    "\n",
    "    ho_obs_pol_proj[ho_obs_states] /= ho_obs_pol_proj[ho_obs_states].sum(axis=-1, keepdims=True)\n",
    "\n",
    "    def projection_func(obs_state_idx):\n",
    "        if obs_state_idx == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return proj_lookup[obs_state_idx]\n",
    "\n",
    "    proj_f = np.vectorize(projection_func)\n",
    "    ho_states_proj = proj_f(ho_states)\n",
    "    assert ho_states_proj.shape == ho_states.shape\n",
    "\n",
    "    ho_obs_samps_proj = utils.format_dgen_samps(\n",
    "       ho_states_proj, ho_actions, ho_rewards, ho_diab, NSTEPS, NHELDOUT)\n",
    "    \n",
    "    this_offpol_opt_reward_WIS_hard_ho, this_ho_wis_samps, this_ho_wis_ct = cf.eval_wis(\n",
    "        ho_obs_samps_proj, discount=DISCOUNT,\n",
    "        bootstrap=False,\n",
    "        obs_policy=ho_obs_pol_proj, new_policy=RlPol)\n",
    "    \n",
    "    obs_reward.append(this_obs_reward)\n",
    "    offpol_opt_reward_WIS_hard_train.append(this_offpol_opt_reward_WIS_hard_train)                       \n",
    "    offpol_opt_reward_WIS_hard_ho.append(this_offpol_opt_reward_WIS_hard_ho)                       \n",
    "    offpol_opt_reward_mb.append(this_offpol_opt_reward_mb)\n",
    "    true_rl_reward.append(this_true_rl_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T17:12:19.355539Z",
     "start_time": "2019-04-19T17:12:19.340377Z"
    }
   },
   "outputs": [],
   "source": [
    "# END OF LOOP\n",
    "def conv_to_np(this_list):\n",
    "    this_arr = np.array(this_list)[:, np.newaxis]\n",
    "    this_arr = this_arr.squeeze()[:, np.newaxis]\n",
    "    return this_arr\n",
    "\n",
    "obs_reward = conv_to_np(obs_reward)\n",
    "offpol_opt_reward_WIS_hard_train = conv_to_np(offpol_opt_reward_WIS_hard_train)\n",
    "offpol_opt_reward_WIS_hard_ho = conv_to_np(offpol_opt_reward_WIS_hard_ho)\n",
    "offpol_opt_reward_mb = conv_to_np(offpol_opt_reward_mb)\n",
    "true_rl_reward = conv_to_np(true_rl_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T18:12:29.839468Z",
     "start_time": "2019-04-19T18:12:29.629901Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "reward = np.concatenate([obs_reward, \n",
    "                         offpol_opt_reward_WIS_hard_train,\n",
    "                         offpol_opt_reward_WIS_hard_ho,                         \n",
    "                         offpol_opt_reward_mb,\n",
    "                         true_rl_reward,\n",
    "                         ], axis=1)\n",
    "reward_df = pd.DataFrame(reward, columns=['Obs', \n",
    "                                          'WIS (train)',\n",
    "                                          'WIS (heldout)',                                          \n",
    "                                          'MB',\n",
    "                                          'True'\n",
    "                                         ])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=reward_df, whis=[2.5, 97.5])\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.savefig(\"{}/{}-ope_wis_mb_cf_true.pdf\".format(\n",
    "    figpath, fig_prefix), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T18:13:58.113270Z",
     "start_time": "2019-04-19T18:13:58.086468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS:\n",
      "\tObserved Reward:  0.3097 \t 95% Range: 0.2694 to 0.3510\n",
      "\tTrue RL Reward:\t -0.1930 \t 95% Range: -0.4051 to -0.0004\n",
      "\tWIS (train) :\t 0.5804 \t 95% Range: -0.2336 to 0.9235\n",
      "\tWIS (heldout) :\t -0.0372 \t 95% Range: -0.9379 to 0.7991\n",
      "\tMB Estimate:\t 0.5779 \t 95% Range: 0.3738 to 0.7270\n"
     ]
    }
   ],
   "source": [
    "print((\"RESULTS:\"\n",
    "       \"\\n\\tObserved Reward:  {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "       \"\\n\\tTrue RL Reward:\\t {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "       \"\\n\\tWIS (train) :\\t {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "       \"\\n\\tWIS (heldout) :\\t {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "       \"\\n\\tMB Estimate:\\t {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "      ).format(\n",
    "    obs_reward.mean(), \n",
    "    np.quantile(obs_reward, 0.025), \n",
    "    np.quantile(obs_reward, 0.975), \n",
    "    true_rl_reward.mean(), \n",
    "    np.quantile(true_rl_reward,0.025), \n",
    "    np.quantile(true_rl_reward, 0.975),\n",
    "    offpol_opt_reward_WIS_hard_train.mean(), \n",
    "    np.quantile(offpol_opt_reward_WIS_hard_train,0.025), \n",
    "    np.quantile(offpol_opt_reward_WIS_hard_train,0.975),\n",
    "    offpol_opt_reward_WIS_hard_ho.mean(), \n",
    "    np.quantile(offpol_opt_reward_WIS_hard_ho,0.025), \n",
    "    np.quantile(offpol_opt_reward_WIS_hard_ho,0.975),\n",
    "    offpol_opt_reward_mb.mean(), \n",
    "    np.quantile(offpol_opt_reward_mb,0.025), \n",
    "    np.quantile(offpol_opt_reward_mb,0.975)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
